#!/usr/bin/env python3
"""
Script to automate the process of generating benchmark reports, visualizations,
and updating the Results.MD file with latest benchmark data.
"""

import os
import sys
import argparse
import datetime
import json
from pathlib import Path

# Add parent directory to path to allow running this script directly
parent_dir = str(Path(__file__).resolve().parent.parent)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from anemll_bench.utils import plot_chip_comparison, plot_benchmark_results


def run_benchmarks(args):
    """Run benchmarks if requested"""
    if args.run_benchmarks:
        print("Running benchmarks on all available models...")
        benchmark_cmd = f"python examples/benchmark_all_models.py --runs {args.runs}"
        if args.no_sync:
            benchmark_cmd += " --no-sync"
        if args.use_local:
            benchmark_cmd += " --use-local"
        result = os.system(benchmark_cmd)
        if result != 0:
            print("Error running benchmarks. Check logs for details.")
            return False
    return True


def generate_visualizations(args):
    """Generate visualization charts"""
    print("Generating visualization charts...")
    
    # Create reports directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Sample data from benchmark results
    # In a real implementation, this would come from parsing the benchmark results file
    chips = ['M1 Max', 'M1 Ultra', 'M2 Max', 'M2 Ultra', 'M4 Pro', 'M4 Max']
    bandwidth = [54.62, 54.72, 62.01, 61.68, 126.36, 118.86]  # GB/s (llama_lm_head)
    inference = [7.61, 7.58, 6.64, 6.70, 3.85, 3.89]           # ms (llama_lm_head_lut6)
    bandwidth_factor = ['1.0x', '1.0x', '1.1x', '1.1x', '2.3x', '2.2x']
    inference_factor = ['1.0x', '1.0x', '1.1x', '1.1x', '2.0x', '2.0x']

    # Create benchmark data dictionary
    benchmark_data = {
        'chips': chips,
        'bandwidth': bandwidth,
        'inference': inference,
        'bandwidth_factor': bandwidth_factor,
        'inference_factor': inference_factor,
    }
    
    # Generate visualization for llama_lm_head model
    output_path = plot_benchmark_results(
        benchmark_data=benchmark_data,
        model_name="llama_lm_head",
        plot_title="ANEMLL-BENCH: Apple Neural Engine Performance Comparison",
        save_dir=args.output_dir,
        show_plot=False
    )
    
    print(f"Generated visualization: {output_path}")
    return benchmark_data


def update_results_md(benchmark_data, args):
    """Update Results.MD with latest benchmark data"""
    print("Updating Results.MD with latest benchmark data...")
    
    results_md_path = os.path.join(parent_dir, "Results.MD")
    
    # Check if Results.MD exists
    if not os.path.exists(results_md_path):
        print("Results.MD not found. Creating new file...")
        
    # Generate Results.MD content
    content = f"""# ANEMLL-Bench Results

This document presents benchmark results for various machine learning models on different Apple Silicon chips, focusing on Neural Engine (ANE) performance.

## Overview

ANEMLL-Bench measures two primary metrics:
1. **Memory Bandwidth (GB/s)**: How Apple Chip Generation utilizes memory bandwidth
2. **Inference Time (ms)**: How quickly the model produces results

Higher memory bandwidth and lower inference time indicate better performance.

## Apple Silicon Performance Comparison

The chart below shows performance comparison across Apple Silicon generations for the `llama_lm_head` model:

![Apple Silicon Performance Comparison](./reports/chip_comparison_llama_lm_head.png)

As shown in the visualization:
- **M4 Series** chips demonstrate approximately 2.3x higher memory bandwidth compared to M1 series
- **M4 Series** inference times are approximately 2.0x faster than M1 series
- The improvements from M1 to M2 were modest (~1.1x), while M4 represents a significant leap

## Detailed Benchmark Results

### llama_lm_head Model (Standard)

| Chip | Memory Bandwidth (GB/s) | Inference Time (ms) | Bandwidth Factor | Inference Factor |
|------|------------------------|---------------------|------------------|------------------|"""

    # Add benchmark data to table
    for i in range(len(benchmark_data['chips'])):
        content += f"""
| {benchmark_data['chips'][i]} | {benchmark_data['bandwidth'][i]} | {benchmark_data['inference'][i]} | {benchmark_data['bandwidth_factor'][i]} | {benchmark_data['inference_factor'][i]} |"""

    content += """

### Key Observations

1. **Neural Engine Scaling**:
   - The M1 Ultra shows minimal performance gains over M1 Max, suggesting limited Neural Engine scaling in first-generation Apple Silicon
   - Similar pattern with M2 Ultra vs M2 Max
   - M4 series demonstrates better scaling and significantly improved performance

2. **Memory Bandwidth Efficiency**:
   - M4 series shows a ~2.3x improvement in memory bandwidth utilization
   - This indicates substantial architectural improvements in the Neural Engine

3. **Inference Time Improvements**:
   - M4 chips process the same model in approximately half the time compared to M1 chips
   - This translates directly to improved user experience for AI applications

## Running Your Own Benchmarks

To reproduce these results or run benchmarks on your own device:

```bash
# Install dependencies
pip install -r requirements.txt
pip install -e .

# Download optimized models for your macOS version
python examples/sync_models.py

# Run benchmarks on all available models
python examples/benchmark_all_models.py

# Generate visualization of results
python examples/plot_chip_comparison.py --save
```

## Contributing Results

We're building a comprehensive benchmark database across all Apple Silicon variants. Please consider submitting your benchmark results by:

1. Running the benchmarks using the instructions above
2. Opening an issue on our GitHub repository with your results
3. Or emailing your results to realanemll@gmail.com

When submitting results, please include:
- Your exact device model (e.g., "MacBook Pro 14" 2023, M3 Pro 12-core CPU, 18-core GPU")
- macOS version
- Any cooling modifications or environmental factors
- The complete benchmark report

## Analyzing Your Results

When analyzing your benchmark results, consider:

1. **Relative Performance**: How does your chip compare to others in the same family?
2. **Scaling Efficiency**: If you have a Pro/Max/Ultra variant, how efficiently does it scale?
3. **Model-Specific Performance**: Different model architectures may perform differently on the same hardware

## Future Work

We plan to expand our benchmarks to include:
- More diverse model architectures
- Power efficiency measurements (performance per watt)
- Sustained performance under thermal constraints
- Newer versions of CoreML and PyTorch

## Acknowledgements

Thanks to all contributors who have submitted benchmark results and helped improve ANEMLL-Bench.

---
*Last updated: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
    
    # Write to Results.MD
    with open(results_md_path, 'w') as f:
        f.write(content)
    
    print(f"Updated {results_md_path}")
    return True


def main():
    parser = argparse.ArgumentParser(description='ANEMLL-Bench Results Generator')
    parser.add_argument('--run-benchmarks', action='store_true', help='Run benchmarks before generating results')
    parser.add_argument('--runs', type=int, default=300, help='Number of benchmark iterations (if running benchmarks)')
    parser.add_argument('--no-sync', action='store_true', help='Skip model synchronization (if running benchmarks)')
    parser.add_argument('--use-local', action='store_true', help='Use only local models (if running benchmarks)')
    parser.add_argument('--output-dir', type=str, default='./reports', help='Directory to save generated files')
    args = parser.parse_args()
    
    # Step 1: Run benchmarks if requested
    if not run_benchmarks(args):
        return
    
    # Step 2: Generate visualizations
    benchmark_data = generate_visualizations(args)
    
    # Step 3: Update Results.MD
    update_results_md(benchmark_data, args)
    
    print("\nResults generation complete! ðŸŽ‰")
    print(f"- Visualizations saved to: {args.output_dir}")
    print(f"- Results.MD updated with latest benchmark data")
    
    if args.run_benchmarks:
        print("\nReminder: Consider submitting your benchmark results to help build our database!")
        print("Email: realanemll@gmail.com or open an issue on GitHub")


if __name__ == "__main__":
    main() 